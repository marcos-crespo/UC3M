---
title: "Statistical Learning Midterm project"
author: "Marcos Crespo"
date: "`r Sys.Date()`"
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: true
    toc_depth: 1
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

library(dplyr)
library(caret)
library(ggplot2)
library(MASS)
library(e1071)
library(plotly)
```

## Introduction

This document is the first half of the Statistical Learning and Machine Learning evaluation assignments for the *Statistical Learning* course for the *MSc in Statistics for Data Science* at *UC3M*. Both parts will compose a full analysis of a selected data.

This first document aims to make a data preprocessing, an EDA, explain the main predictors affecting the output and try some classification using purely statistical learning tools. The second document objectives will be predict the output using some machine learning tools. The differences between statistical learning and machine learning were conveniently explained during de course.

## The data

The selected data is [*Salary structure survey*](https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177025&menu=resultados&secc=1254736195110&idp=1254735976596#!tabs-1254736195110). The data was publicly collected and published by the Spanish National Institute of Statistics [(INE)](https://www.ine.es/index.htm) anonymized microdata program. The data is collected every 4 years and the latest data available is 2018 survey.

In this survey various information about the salary of the respondents was collected. Higher education, working hours, place of work, flexible remuneration or sector are some examples. The database is really complex and large but the INE provides two very important documents that help us to better understand it. The first is an .xlxs document which explains all the fields collected by the survey, their meaning and possible values. The second is a text document in which they explain some interesting metrics that can be calculated with the collected data. We will use both for our analysis.

If all the specific fields and its description wants to be consulted, please check the .xlsx file in the folder you download [here](https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177025&menu=resultados&secc=1254736195110&idp=1254735976596#!tabs-1254736195110). The data base consists of 216726 entries of 56 variables.

### Data preprocessing

When you download the data from the source, you get a directory called R. In this directory there are instructions in how to make the given metadata as a R data frame. This code was quite outdated so we have updated it and let it in the Annex [**GET REFERENCE AND INSERT ANNEX**]. The code combines the .xlsx metadata file explaining all variables with the .txt file containing the info and lastly produces an R data frame with all the data in the correct format.

```{r fichero_salida}
setwd("C:/Users/marco/Documents/Universidad/Master/GitHub/UC3M/Statistical Learning/MidTerm/datos_2018/R")
source("MD_EES_2018.R")
```


Once we have our data frame we are going to add some of the important metrics the INE recommends.

First we notice that the data was collected in both monthly and annual basis. The monthly information belongs to October and the annual is the complete information for 2018. We will get rid of all the variables for monthly information and make our study for the full year. 

```{r train_df}
# Change the name of the dataframe
df <- fichero_salida

# Drop specified columns
df <- df[, !names(df) %in% c("DRELABM", "SIESPM1", "DSIESPM1", "SIESPM2", "ORDENTRA", 
                                      "SALBASE", "EXTRAORM", "PHEXTRA", "COMSAL", "COMSALTT",
                                      "IRPFMES", "COTIZA", "BASE")]

```

In addition we will add a column representing the full annual salary combining the different types of retribution each individual gets. The formula the INE gives us using column names is:

$SALANUAL=(365/DIASANO)*(RETRINOIN+RETRIIN+VESPNOIN+VESPIN)$

Note that several metrics will be also created:

- $DIASANO=DIASRELABA-DSIESPA2-DSIESPA4$
- $VESPNOIN=(365/DIASANO)*VESPNOIN$
- $VESPIN=(365/DIASANO)*VESPIN$
- $DIASRELABA=DRELABAM*30.42+DRELABAD$[^1]

[^1]:Adjust: IF DIASRELABA >365 THEN DIASRELABA =365)

We will need to take the created variables out of the scope of the analysis because the response variable is a linear combination of them. *SALANUAL* will be our response variable, the one we will be trying to explain and predict using all the other information. 

Finally, some other non informative variables will be dropped:

- FACTOTAL: The elevation factor, an statistical metric.
- ANOANTI and MESANTI are redundant. They stand for the time anyone has worked in years and in months respectively. MESANTI only is different from 0 if ANOANTI is 0 so we can make everything in months.
- IDENCCC corresponds to a unique quote index. It has to be ignored.

```{r SALANUAL}
# Calculate the new columns

df$DIASRELABA <- df$DRELABAM * 30.42 + df$DRELABAD
# Adjust
df$DIASRELABA[df$DIASRELABA > 365] <- 365


df$DIASANO <- df$DIASRELABA - df$DSIESPA2 - df$DSIESPA4

# Calculate VESPNOIN and VESPIN
df$VESPNOIN <- (365 / df$DIASANO) * df$VESPNOIN
df$VESPIN <- (365 / df$DIASANO) * df$VESPIN


df$SALANUAL <- (365 / df$DIASANO) * (df$RETRINOIN + df$RETRIIN + df$VESPNOIN + df$VESPIN)

# Drop specified columns
df <- df[, !names(df) %in% c("DIASRELABA", "DIASANO", "VESPNOIN", "VESPIN", "RETRINOIN","RETRIIN" , "FACTOTAL", "IDENCCC")]

# Multiply ANOANTI by 12 where ANOANTI is not equal to 0
df <- df %>%
  mutate(MESANTI = ifelse(ANOANTI != 0, ANOANTI * 12, MESANTI)) %>%
  # Drop the MESANTI variable
  dplyr::select(-ANOANTI)


```


Finally, for most of our techniques, we will prefer not to predict some continuous variable as *SALANUAL* is, but rather a categorical one with several categories. 
For the OECD[^2], the lower class in Spain is the one whose income is below 75% of the national median income; the middle class is between 75% and 200% of the median, and the upper class is the one above 200% of the median. According to the OECD estimate for 2019 (around that time), the median income is at $15193$ euros per year. Therefore, by applying the formulas, lower class people would be those who receive less than $11395$ euros per year; middle class people are between that figure and $30386$ euros per year, while the upper class includes all those who receive more than thirty thousand euros per year.

[^2]: Information in this [article](https://www.larazon.es/economia/eres-clase-alta-media-baja-esta-categoria-que-perteneces-sueldo-segun-ocde_20240109659d9f0d872b820001237c5a.html)

This way we can define three classes like:
```{r classes}
# Define the thresholds
thresholds <- c(-Inf, 11395, 30386, Inf)

# Create SALANUAL_CAT variable with three categories
df <- df %>%
  mutate(SALANUAL_CAT = cut(SALANUAL, breaks = thresholds, labels = c("Low Class", "Middle Class", "High Class")))
```

The resulting classes are:

```{r}
table(df$SALANUAL_CAT)
```


So they are pretty balanced. No correction for imbalances will be needed

Now we are ready to create our train-test partition and begin the analysis:

```{r train_test}
# Set the seed for reproducibility
set.seed(123)

# Create the training and testing sets with a 80/20 split
train_df <- createDataPartition(df$SALANUAL_CAT, p = 0.8, list = FALSE, times = 1)
df_train <- df[train_df, ]
df_test <- df[-train_df, ]

```


## Exploratory Data Analysis {.tabset}

With all the preprocessing done we can now see some characteristics of our data.

```{r table1, echo=FALSE}
rmarkdown::paged_table(head(df_train))
```
### Tables and plots

In *CNACE* we get a code representing the *ECONOMIC ACTIVITY CODE*, it means, the economic sector in which each individual works. We have the following individuals:
```{r cnace_table, echo=FALSE}
table(df_train$CNACE)
```
The biggest categories are N0,Q0 and M0. If we consult the [CNAE 2009 table](https://www.cnae.com.es/lista-actividades.php), table that is copied in the informative .xlsx file that comes with the survey data we see this categories correspond to mostly qualified services jobs. This means jobs in administration, science, finance, consultancy, tourism, health...

Even though there are a lot of categories we could try and see if some of these categories tend to have more salary than others. 

```{r cnace_plot, echo=FALSE}
# Create density plot
p<-ggplot(df_train, aes(color = CNACE, fill = CNACE)) +
  geom_density(alpha = 0.6, aes(x = log(SALANUAL))) +
  labs(x = "SALANUAL", y = "Density", color = "CNACE") +
  ggtitle("Density Plot of SALANUAL by CNACE") +
  theme_minimal()

ggplotly(p)
```

Here we can see how maybe D0 is slightly right deviated meaning people working in the Energy industry tend to earn more.

We can also see the distribution by sex:

```{r plot_sex, echo=FALSE}

g<-ggplot(df_train) +
  geom_density(alpha = 0.5, aes(x = log(SALANUAL+1),color = factor(df_train$SEXO, levels = c(1, 6), labels = c("Men", "Women")))) +
  labs(x = "Annual Salary", y = "Density", color = "Sex", fill = "Sex") +
  ggtitle("Density Plot of SALANUAL and Sex") +
  theme_minimal()

ggplotly(g)
```

Men tend to earn slightly more in mean. We could also check the number of men ad women in high-class income:

```{r}
high_class_table <- df_train %>%
  filter(SALANUAL_CAT=="High Class") %>%
  group_by(SEXO) %>%
  summarise(high_class_count = n())

print(high_class_table)

```
So even though the men only tend to earn a little bit more in mean than women,it is almost twice as likely to be high class if you are a man.


Also we could try to see if the time spent in a job make you earn more in mean. And also check for this by sex

```{r}
# Create scatter plot
t<-ggplot(df_train, aes(x = MESANTI, y = SALANUAL, color = SEXO)) +
  geom_line() +
  labs(x = "MESANTI", y = "SALANUAL", color = "Sex") +
  ggtitle("Scatter Plot of SALANUAL by MESANTI and Sex") +
  theme_minimal()

ggplotly(t)
```
Here we can see how not only the number of months spent in a job doesn't make you earn more, but it seems that the highest earning jobs are not very old jobs.

A similar addition to the sector analysis may be interesting but with education. This is:

```{r cnace_plot2, echo=FALSE}
# Create density plot
f<-ggplot(df_train, aes(color = ESTU, fill = ESTU)) +
  geom_density(alpha = 0.6, aes(x = log(SALANUAL))) +
  labs(x = "SALANUAL", y = "Density", color = "ESTU") +
  ggtitle("Density Plot of SALANUAL by education") +
  theme_minimal()

ggplotly(f)
```
Here, the deviation is pretty clear. Category *1* stands form *Less than primary* and its clearly the more left deviated so smaller category. Category *7* stands for Bachelors and similar, and university doctors and they are the highest paid in average.


## Statistical Learning Models {.tabset}

We will be using library `caret` for our analysis due to its ease of use and its cross validation capabilities. Our approach will be making a 5-fold cross validation for all the models and then making an accuracy comparison for every model.

```{r cv, echo=FALSE}
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 1,
                     number = 5)
```

### Bayesian classifiers

A Bayesian classifier is a method used for classification tasks, based on Bayes' theorem. It calculates the probability of a hypothesis being true given the evidence. In simpler terms, it works by looking at the probability of a data point belonging to a certain class given its features. The classifier then selects the class with the highest probability as the predicted label for the input. It's a powerful tool because it can incorporate prior knowledge about the data and update its predictions as it receives new information.

In the course, we have been presented 3 Bayesian classifiers techniques: QDA, LDA and Naive Bayes. Since QDA is not recommended for high dimensional mixed-type data sets, we will only perform LDA (recommended for multi-class classification) and Naive Bayes (recommended for large data sets) classifiers.


```{r lda, echo=FALSE}
lda<- caret::train(SALANUAL_CAT~.-SALANUAL, data = df_train, method="lda",preProcess = c("center", "scale"), trControl=ctrl)
```
```{r nb}
nb<-caret::train(SALANUAL_CAT~.-SALANUAL, data = df_train, method="naive_bayes",preProcess = c("center", "scale"), trControl=ctrl)

```

### LOGISTIC REGRESSION

Multinomial logistic regression is a statistical method used to predict the probability of multiple categorical outcomes. Unlike binary logistic regression, which deals with only two categories, multinomial logistic regression handles three or more categories simultaneously. It estimates separate sets of coefficients for each outcome category relative to a reference category, describing the relationship between predictor variables and the likelihood of belonging to each category. The model's output provides probabilities for each outcome category, allowing us to classify observations into the most likely category based on the values of the predictor variables.

```{r lr}
lr <- train(SALANUAL_CAT~.-SALANUAL, 
                method = "glmnet",
                family = "multinomial",
                data = df_train,
                preProcess = c("center", "scale"),
                metric = "Accuracy", trControl=ctrl)
```
```{r}
lr_imp <- varImp(lr, scale = F)
plot(lr_imp, scales = list(y = list(cex = .95)))
```

## Results


```{r predict}
lda.pred<-predict(lda,newdata = df_test)
nb.pred <- predict(nb, newdata = df_test)
lr.pred<- predict(lr, newdata = df_test)

conf_matrix1 <- confusionMatrix(lda.pred, df_test$SALANUAL_CAT)
conf_matrix2 <- confusionMatrix(nb.pred, df_test$SALANUAL_CAT)
conf_matrix3 <- confusionMatrix(lr.pred, df_test$SALANUAL_CAT)

# Extract accuracy metric
conf_matrix1$overall['Sensitivity']
conf_matrix2$overall['Sensitivity']
conf_matrix3$overall['Sensitivity']

```
COMENTAR pq usas sensitivity (True positive rate). no quiero beneficiar que ayude el clasificar bien un negative. quiero solo ver cuantos de cada grupo he clasificado bien.

## Annex


