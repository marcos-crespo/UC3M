cite(dplyr)
cite("dplyr")
citation("dplyr")
knitr::opts_chunk$set(echo = TRUE)
load("qmnist_nist.RData")
# dplyr
if (!requireNamespace("dplyr", quietly = TRUE)) {
install.packages("dplyr")
}
library(dplyr)
# glmnet
if (!requireNamespace("glmnet", quietly = TRUE)) {
install.packages("glmnet")
}
library(glmnet)
# Filter our original data
train <- train_nist %>%
filter(digit %in% c(4, 9))
train$digit <- droplevels(train$digit)
cor(train$px)
a<-cor(train$px)
corrplot::corrplot(a)
summary(train$PX)
summary(train$px$1)
summary(train$px[1])
View(train)
View(train)
cor(train$px[1], train$px[2])
cor(train$px[3], train$px[4])
cor(train$px[50], train$px[55])
cor(train$px[80], train$px[160])
cor(train$px[400], train$px[420])
sd(train$px[1], train$px[2])
var(train$px[1], train$px[2])
# 3-fold cross-validation. Change the seed for a different result
set.seed(12345)
kcvRidge <- cv.glmnet(x = as.matrix(train[,3]), y = train[,1], alpha = 0, standardize = FALSE, family = "binomial", nfolds = 3)
# The lambda that minimizes the CV error is
kcvRidge$lambda.min
# Equivalent to
indMin <- which.min(kcvRidge$cvm)
kcvRidge$lambda[indMin]
# The minimum CV error
kcvRidge$cvm[indMin]
## [1] 0.1163279
min(kcvRidge$cvm)
# Potential problem! Minimum occurs at one extreme of the lambda grid in which
# CV is done. The grid was automatically selected, but can be manually inputted
range(kcvRidge$lambda)
# 3-fold cross-validation. Change the seed for a different result
set.seed(12345)
kcvRidge <- cv.glmnet(x = as.matrix(train[,3]), y = train[,1], alpha = 0, standardize = FALSE, family = "binomial", nfolds = 3)
# The lambda that minimizes the CV error is
kcvRidge$lambda.min
citation(glmnet)
citation("glmnet")
'print(citation("glmnet"), bibtex=TRUE)
3
f
''
``
'
print(citation("glmnet"), bibtex=TRUE)
# 10-fold cross-validation. Keep the seed for reproducibility
set.seed(12345)
kcvRidge <- cv.glmnet(x = as.matrix(train$px), y = train$digit, alpha = 0, standardize = FALSE, family = "binomial", nfolds = 10)
kcvRidge$lambda.min
kcvRidge$lambda.1se
plot(kcvRidge)
range(kcvRidge$lambda)
abline(h = kcvRidge$cvm[indMin] + c(0, kcvRidge$cvsd[indMin]))
lambdaGrid <- 10^seq(log10(kcvRidge$lambda[1]), log10(0.1),length.out = 150)
kcvRidge <- cv.glmnet(x = as.matrix(train$px), y = train$digit, alpha = 0, standardize = FALSE, family = "binomial", nfolds = 10)
kcvRidge <- cv.glmnet(x = as.matrix(train$px), y = train$digit, alpha = 0, standardize = FALSE, family = "binomial", nfolds = 10, lambda = lambdaGrid)
range(kcvRidge$lambda)
kcvRidge$lambda.1se
kcvRidge$lambda.min
plot(kcvRidge)
ridgeMod <- glmnet(x = x, y = y, alpha = 0, lambda = 83.64)
ridgeMod <- glmnet(x = as.matrix(train$px), y = train$digit, alpha = 0, family = "binomial", standardize = FALSE ,lambda = 83.64)
# Location of both optimal lambdas in the CV loss function in dashed vertical
# lines, and lowest CV error and lowest CV error + one standard error
plot(ridgeMod)
# Location of both optimal lambdas in the CV loss function in dashed vertical
# lines, and lowest CV error and lowest CV error + one standard error
plot(ridgeMod, xvar = "norm", label = TRUE)
plot(ridgeMod, label = TRUE, xvar = "lambda")
# Location of both optimal lambdas in the CV loss function in dashed vertical
# lines, and lowest CV error and lowest CV error + one standard error
plot(ridgeMod, xvar = "norm", label = TRUE)
plot(coef(ridgeMod, s = "lambda.1se")[-1])
plot(coef(ridgeMod)[-1])
plot(coef(ridgeMod))
plot(coef(ridgeMod)[-1])
predictions <- predict(ridgeMod, type = "response",
newx = as.matrix(test$px))
# Filter our original data
test <- test_nist %>%
filter(digit %in% c(4, 9))
test$digit <- droplevels(test$digit)
test$digit <- droplevels(test$digit)
We can now use the test set (properly transformed and filtered) in order to make some predictions using the model and measure the performance of the model. We are using a logistic binomial model, so the prediction is a estimated probability of the event being a certain class. Since we want a binomial output for computing the accuracy of the prediction, a cut-off for the probability is needed. The selected cut-off will be the naive $0.5$.
```{r predict}
predictions <- predict(ridgeMod, type = "response",
newx = as.matrix(test$px))
# Convert predicted probabilities to class labels (4 or 9)
predicted_classes <- ifelse(predictions > 0.5, 9, 4)
# Evaluate the accuracy
accuracy <- mean(predicted_classes == test$digit)
lambdaGrid <- 10^seq(log10(kcvRidge$lambda[1]), log10(0.1),length.out = 150)
lambdaGrid
range(10^seq(log10(kcvRidge$lambda[1]), log10(0.1),length.out = 150))
range(10^seq(log10(kcvRidge$lambda[1]), log10(0.1),length.out = 150)
)
10^seq(log10(45193.6), log10(0.1),length.out = 150)
range(10^seq(log10(45193.6), log10(0.1),length.out = 150))
# Example datasets
dataset1 <- data.frame(x = 1:10, y = 2*(1:10) + rnorm(10))
dataset2 <- data.frame(x = 1:10, y = 3*(1:10) + rnorm(10))
# Combine datasets into a list
datasets <- list(dataset1, dataset2)
# Vectorized linear model fitting
models <- lapply(datasets, function(data) lm(y ~ x, data = data))
# Accessing coefficients of each model
for (i in 1:length(models)) {
cat("Coefficients for Dataset", i, ":\n")
print(coef(models[[i]]))
}
# Example dataset
df <- data.frame(digits = sample(0:9, 100, replace = TRUE),
value = rnorm(100))  # Assuming your dataset has another column named 'value'
# Function to create pairwise combinations
pairwise_combinations <- function(digits_vec) {
comb <- combn(digits_vec, 2)
comb <- t(comb)
comb
}
# Get unique values in 'digits' column
unique_digits <- unique(df$digits)
# Create pairwise combinations
combinations <- pairwise_combinations(unique_digits)
# Split the dataset based on 'digits' column
split_datasets <- split(df, df$digits)
# Filter datasets for each pairwise combination
pairwise_datasets <- lapply(1:ncol(combinations), function(i) {
digit1 <- combinations[1, i]
digit2 <- combinations[2, i]
dataset1 <- split_datasets[[as.character(digit1)]]
dataset2 <- split_datasets[[as.character(digit2)]]
filtered_dataset <- rbind(dataset1, dataset2)
filtered_dataset
})
# Example of accessing pairwise dataset for combination of digits 0 and 1
pairwise_datasets[[1]]  # Replace '1' with the index corresponding to the desired combination
0:9
datasets <- list()
glmnet(formula= digits~px,data = train, alpha=0, standardize = FALSE, family = "binomial"))
glmnet(formula= digits~px,data = train, alpha=0, standardize = FALSE, family = "binomial")
View(test)
lev <- 0:9
datasets_train <- list(45)
datasets_test<- list(45)
cont<- 1
for (i in lev) {for (j in lev) {if (i < j) {
train_l <- train_nist %>%
filter(digit %in% c(i, j))
train_l$digit <- droplevels(train_l$digit)
test_l <- test_nist %>%
filter(digit %in% c(i, j))
test_l$digit <- droplevels(test_l$digit)
print(cont)
datasets_train[cont] <- train_l
datasets_test[cont] <- test_l
cont<-cont+1
}}}
models <- lapply(datasets_train, function(data) {
x <- as.matrix(data$px)
y <- data$digits
kcvRidge <- cv.glmnet(x,y, alpha = 0, standardize = FALSE, family = "binomial", nfolds = 10)
})
models <- lapply(datasets_train, function(data) {
x <- as.matrix(data[,3])
y <- data[,1]
kcvRidge <- cv.glmnet(x,y, alpha = 0, standardize = FALSE, family = "binomial", nfolds = 10)
})
View(datasets_train)
View(datasets_train)
lev <- 0:9
datasets_train <- list(45)
datasets_test<- list(45)
View(datasets_test)
View(datasets_test)
datasets_train <- list(length=45)
View(datasets_test)
View(datasets_test)
datasets_train <- list(length=45)
datasets_train <- vector(length=45)
lev <- 0:9
datasets_train <- vector(length=45)
datasets_test<- vector(length=45)
cont<- 1
for (i in lev) {for (j in lev) {if (i < j) {
train_l <- train_nist %>%
filter(digit %in% c(i, j))
train_l$digit <- droplevels(train_l$digit)
test_l <- test_nist %>%
filter(digit %in% c(i, j))
test_l$digit <- droplevels(test_l$digit)
print(cont)
datasets_train[cont] <- train_l
datasets_test[cont] <- test_l
cont<-cont+1
}}}
View(datasets_test)
View(datasets_test)
datasets_test[[2]]
models <- lapply(datasets_train, function(data) {
x <- as.matrix(data[,3])
y <- data[,1]
kcvRidge <- cv.glmnet(x,y, alpha = 0, standardize = FALSE, family = "binomial", nfolds = 10)
})
datasets_test[2]
View(train_l)
uwu<- vector("list", length = 45)
View(uwu)
uwu[45]<-train_l
uwu[45]<-as.df(train_l)
uwu[45]<-df(train_l)
class(train_l)
uwu<- vector("list", length = 45)
lev <- 0:9
datasets_train <- vector(length=45)
datasets_test<- vector(length=45)
cont<- 1
for (i in lev) {for (j in lev) {if (i < j) {
train_l <- train_nist %>%
filter(digit %in% c(i, j))
train_l$digit <- droplevels(train_l$digit)
test_l <- test_nist %>%
filter(digit %in% c(i, j))
test_l$digit <- droplevels(test_l$digit)
print(cont)
datasets_train[[cont]] <- train_l
datasets_test[[cont]] <- test_l
cont<-cont+1
}}}
lev <- 0:9
datasets_train <- vector("list",length=45)
datasets_test<- vector("list", length=45)
cont<- 1
for (i in lev) {for (j in lev) {if (i < j) {
train_l <- train_nist %>%
filter(digit %in% c(i, j))
train_l$digit <- droplevels(train_l$digit)
test_l <- test_nist %>%
filter(digit %in% c(i, j))
test_l$digit <- droplevels(test_l$digit)
print(cont)
datasets_train[[cont]] <- train_l
datasets_test[[cont]] <- test_l
cont<-cont+1
}}}
View(datasets_test)
View(datasets_test)
models <- lapply(datasets_train, function(data) {
x <- as.matrix(data[,3])
y <- data[,1]
kcvRidge <- cv.glmnet(x,y, alpha = 0, standardize = FALSE, family = "binomial", nfolds = 10)
})
predictions <- mapply(function(model, new_data) {
x_new <- as.matrix(new_data$px)
predict(model, newx = x_new,  s = "lambda.1se", type = "response")
}, models, datasets_test)
# Vectorized class predictions and accuracy calculation
accuracy <- sapply(predictions, function(pred, y_test) {
class_predictions <- ifelse(pred > 0.5, 1, 0)
mean(class_predictions == y_test)
}, y_test = datasets_test$digit)
# Print accuracy for each dataset
print(accuracy)
View(predictions)
View(predictions)
# Vectorized class predictions and accuracy calculation
accuracy <- mapply(function(pred, y_test) {
class_predictions <- ifelse(pred > 0.5, 1, 0)
mean(class_predictions == y_test)
},predictions,  y_test = datasets_test$digit)
# Print accuracy for each dataset
print(accuracy)
accuracy
# Vectorized class predictions and accuracy calculation
accuracy <- mapply(function(pred, test) {
y_test <- test$digit
class_predictions <- ifelse(pred > 0.5, 1, 0)
mean(class_predictions == y_test)
},predictions,datasets_test)
accuracy
factor(train$digit)
model.matrix(factor(train$digit))
model.matrix(train$digit)
model.matrix(train[,1])
train[,1]
model.matrix(~ factor(train$digit))
model.matrix(~ factor(train$digit))[,2]
as.vector(model.matrix(~ factor(train$digit))[,2])
# Vectorized class predictions and accuracy calculation
accuracy <- mapply(function(pred, mytest) {
y_test <- as.vector(model.matrix(~ factor(mytest$digit))[,2])
class_predictions <- ifelse(pred > 0.5, 1, 0)
mean(class_predictions == y_test)
},predictions,datasets_test)
accuracy
which.max(accuracy)
which.min(accuracy)
# Crear una matriz triangular inferior con los elementos del vector
matriz_triangular <- matrix(0, nrow = 10, ncol = 10)
matriz_triangular[lower.tri(matriz_triangular)] <- accuracy
# Mostrar la matriz resultante
print(matriz_triangular)
# Crear una matriz triangular inferior con los elementos del vector
matriz_triangular <- matrix(0, nrow = 10, ncol = 10)
matriz_triangular[upper.tri(matriz_triangular)] <- accuracy
# Mostrar la matriz resultante
print(matriz_triangular)
# Crear una matriz triangular inferior con los elementos del vector
matriz_triangular <- matrix(0, nrow = 10, ncol = 10)
matriz_triangular[upper.tri(matriz_triangular)] <- accuracy
rownames(matriz_triangular) <- colnames(matriz_triangular) <- 0:9
# Mostrar la matriz resultante
print(matriz_triangular)
library(ggplot2)
# Assuming ridgeMod is your ridge regression model
coefficients <- coef(ridgeMod)[-1]  # Extracting coefficients
# Creating a data frame with coefficients and their indices
coefficients_df <- data.frame(index = 1:length(coefficients),
coefficient = coefficients)
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point() +
labs(x = "Index", y = "Coefficient") +
ggtitle("Scatter Plot of Ridge Regression Coefficients")
library(ggplot2)
# Assuming ridgeMod is your ridge regression model
coefficients <- coef(ridgeMod)[-1]  # Extracting coefficients
# Creating a data frame with coefficients and their indices
coefficients_df <- data.frame(index = 1:length(coefficients),
coefficient = coefficients)
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "lightgreen") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme(panel.background = element_rect(fill = "transparent"))  # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "green") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme(panel.background = element_rect(fill = "transparent"))  # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "blue") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme(panel.background = element_rect(fill = "transparent"))  # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "blue") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "cyan") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "grey") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "lightyellow") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "yellow") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "brown") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "orange") +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "orange", size=1.5) +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
# Plotting using ggplot2
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "orange", size=.5) +  # Change dot color to light green
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
show_digit <- function(x, col = gray(255:1 / 255), ...) {
l <- sqrt(length(x))
image(matrix(as.numeric(x), nrow = l)[, l:1], col = col, ...)
}
show_digit(train_nist[1,3])
show_digit <- function(x, col = gray(255:1 / 255), ...) {
l <- sqrt(length(x))
image(matrix(as.numeric(x), nrow = l)[, l:1], col = col,axes=FALSE)
}
show_digit(train_nist[1,3])
par(mar = c(4, 4, .2, .1))
coefficients <- coef(ridgeMod)[-1]  # No intercept
coefficients_df <- data.frame(index = 1:length(coefficients),
coefficient = coefficients)
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "orange", size = .5) +
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
show_digit <- function(x, col = gray(255:1 / 255), ...) {
l <- sqrt(length(x))
image(matrix(as.numeric(x), nrow = l)[, l:1],
col = col,
axes = FALSE)
}
show_digit(train_nist[1, 3])
show_digit <- function(x, col = gray(255:1 / 255), ...) {
l <- sqrt(length(x))
image(matrix(as.numeric(x), nrow = l)[, l:1],
col = col,
axes = FALSE)
}
show_digit(train_nist[1, 3])
par(mar = c(4, 4, .2, .1))
coefficients <- coef(ridgeMod)[-1]  # No intercept
coefficients_df <- data.frame(index = 1:length(coefficients),
coefficient = coefficients)
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "orange", size = .5) +
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
par(mar = c(4, 4, .2, .1))
show_digit <- function(x, col = gray(255:1 / 255), ...) {
l <- sqrt(length(x))
image(matrix(as.numeric(x), nrow = l)[, l:1],
col = col,
axes = FALSE)
}
show_digit(train_nist[1, 3])
coefficients <- coef(ridgeMod)[-1]  # No intercept
coefficients_df <- data.frame(index = 1:length(coefficients),
coefficient = coefficients)
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "orange", size = .5) +
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
par(mar = c(1, 2, .2, .1))
show_digit <- function(x, col = gray(255:1 / 255), ...) {
l <- sqrt(length(x))
image(matrix(as.numeric(x), nrow = l)[, l:1],
col = col,
axes = FALSE)
}
show_digit(train_nist[1, 3])
coefficients <- coef(ridgeMod)[-1]  # No intercept
coefficients_df <- data.frame(index = 1:length(coefficients),
coefficient = coefficients)
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "orange", size = .5) +
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
show_digit <- function(x, col = gray(255:1 / 255), ...) {
l <- sqrt(length(x))
image(matrix(as.numeric(x), nrow = l)[, l:1],
col = col,
axes = FALSE)
}
show_digit(train_nist[1, 3])
coefficients <- coef(ridgeMod)[-1]  # No intercept
coefficients_df <- data.frame(index = 1:length(coefficients),
coefficient = coefficients)
ggplot(coefficients_df, aes(x = index, y = coefficient)) +
geom_point(color = "orange", size = .5) +
labs(x = "Index", y = "Coefficient") +
ggtitle("Ridge Regression Coefficients") +
theme_light() # Set background to transparent
